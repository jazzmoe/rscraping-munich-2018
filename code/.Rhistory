# word cloud of 100 most common tags (DON'T do this)
textplot_wordcloud(tweet_dfm, max_words = 100)
names(docvars(tweet_corp))
names(docvars(twcorpus))
table(docvars(twcorpus)$screen_name)
# create document-level variable indicating whether Tweet was in English or other language
docvars(twcorpus, "dummy_bernie") <- factor(ifelse(docvars(twcorpus, "screen_name") == "BernieSanders", "Bernie", "Not Bernie"))
tweet_corp_bernie <- dfm(twcorpus, select = "#*", groups = "dummy_bernie")
# wordcloud with group comparison (again: DON'T do this)
textplot_wordcloud(tweet_corp_bernie, comparison = TRUE, max_words = 100)
twdfm <- dfm(twcorpus, groups=c("screen_name"), remove_punct=TRUE,
remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target="realDonaldTrump",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="HillaryClinton",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="tedcruz",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="BernieSanders",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="realDonaldTrump",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="HillaryClinton",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="tedcruz",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="BernieSanders",
measure="chi2"), n=5)
trump <- corpus_subset(twcorpus, screen_name=="realDonaldTrump")
twdfm <- dfm(trump, remove_punct=TRUE,
remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month<"2016-01",
measure="chi2"), n=5)
trump <- paste(
tweets$text[tweets$screen_name=="realDonaldTrump"], collapse=" ")
textplot_xray(kwic(trump, "hillary"), scale="relative")
textplot_xray(kwic(trump, "crooked"), scale="relative")
textplot_xray(kwic(trump, "mexic*"), scale="relative")
textplot_xray(kwic(trump, "fake"), scale="relative")
textplot_xray(kwic(trump, "immigr*"), scale="relative")
textplot_xray(kwic(trump, "immigr*"), scale="relative")
textplot_xray(kwic(trump, "muslim*"), scale="relative")
clinton <- paste(
tweets$text[tweets$screen_name=="HillaryClinton"], collapse=" ")
textplot_xray(kwic(clinton, "trump"), scale="relative")
textplot_xray(kwic(clinton, "sanders"), scale="relative")
textplot_xray(kwic(clinton, "gun*"), scale="relative")
load("../data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
# select tokens starting with capital letters
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
cap_col[1]
# identify and score multi-word expressions (with at least 10 observations)
cap_col <- textstat_collocations(cap_col, min_count = 10)
head(cap_col, 20)
# use results from analysis to compound meaningful (statistically strongly associated) tokens; this makes them less ambiguous and might improve statistical analyses
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
comp_toks[1]
# Lexicoder Sentiment Dictionary by Young/Soroka
browseURL("https://www.tandfonline.com/doi/abs/10.1080/10584609.2012.671234")
# data available in quanteda!
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
# load Guardian corpus
load("../data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
class(guardianSample)
guardianSample$tokens
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# check sentiment
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
# more targeted sentiment analysis
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks[2]
# grouped DFM
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
# plot absolute frequencies
matplot(eu_lsd_dfm, type = 'l', xaxt = 'n', lty = 1, ylab = 'Frequency')
grid()
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
legend('topright', col = 1:2, legend = c('Negative', 'Positive'), lty = 1, bg = 'white')
# plot average sentiment
eu_n <- ntoken(dfm(eu_toks, group = docvars(eu_toks, 'week')))
plot((eu_lsd_dfm[,2] - eu_lsd_dfm[,1]) / eu_n,
type = 'l', ylab = 'Sentiment', xlab = '', xaxt = 'n')
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
grid()
abline(h = 0, lty = 2)
# load data
uk <- read.csv("../data/FB-UK-parties.csv", stringsAsFactors = FALSE)
head(uk)
table(uk$party)
# build dictionary
populist_dict <- dictionary(list(
populism = c(
"elit*",
"consensus*",
"undemocratic*",
"referend*",
"corrupt*",
"propagand*",
"politici*",
"*deceit*",
"*deceiv*",
"*betray*",
"shame*",
"scandal*",
"truth*",
"dishonest*",
"establishm*",
"ruling*")))
# create corpus
fbcorpus <- corpus(uk)
fbdfm <- dfm(fbcorpus, groups = "party")
# normalize for document length: turn word counts into proportions
fbdfm <- dfm_weight(fbdfm, scheme="prop")
# find % of words in populism dictionary
pop <- dfm_lookup(fbdfm, dictionary = populist_dict)
pop * 100
# check precision
kwic(fbcorpus, pattern = 'undemocratic') # sounds good
kwic(fbcorpus, pattern = 'ruling*') # probably not
# check recall
kwic(fbcorpus, pattern = 'unaccountable')
kwic(fbcorpus, pattern = 'dodging')
?xgb.cv
?xgboost
## peparations -------------------
source("packages.r")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
?dgCMatrix
class(X)
# parameters to explore
tryEta <- c(.1, .3, .5)
tryDepths <- c(1, 3, 6)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
## regularized regression ----------
# random sample of nearly 5,000 tweets mentioning the names of the candidates to the 2014 EP elections in the UK.
# focus on variable named `communication`, which indicates whether each tweet was hand-coded as being __engaging__ (a tweet that tries to engage with the audience of the account) or __broadcasting__ (just sending a message, without trying to elicit a response)
# source: Yannis Theocharis, Pablo Barber√°, Zoltan Fazekas, and Sebastian Popa,, Journal of Communication. (http://onlinelibrary.wiley.com/doi/10.1111/jcom.12259/abstract).
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors=F)
tweets$engaging <- ifelse(tweets$communication=="engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out it can be quite informative.
twdfm <- dfm(twcorpus, remove=stopwords("english"), remove_url=TRUE,
ngrams=1:2, verbose=TRUE) #
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
# parameters to explore
tryEta <- c(.1, .3, .5)
tryDepths <- c(3, 6)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
summary(data_corpus_inaugural)
source("packages.r")
summary(data_corpus_inaugural)
docvars(sotu_corpus)
docvars(data_corpus_inaugural)
docvars(data_corpus_inaugural) %>% head()
docvars(data_corpus_inaugural)$foo <- 1
docvars(data_corpus_inaugural) %>% head()
docvars(data_corpus_inaugural)$FullName <- paste(docvars(data_corpus_inaugural)$FirstName, docvars(data_corpus_inaugural)$President)
docvars(data_corpus_inaugural) %>% head()
summary(sotu_corpus, 5)
summary(data_corpus_inaugural, 5)
# subset corpus
str(data_corpus_inaugural) # structure of corpus object
# add metadata
docvars(data_corpus_inaugural) %>% head()
corpus_inaugural_sub <- corpus_subset(data_corpus_inaugural, Year >= 2000)
summary(corpus_inaugural_sub)
# change units of texts (documents, paragraphs, sentences)
ndoc(corpus_inaugural_sub)
# change units of texts (documents, paragraphs, sentences)
ndoc(data_corpus_inaugural)
corpus_inaugural_sent <- corpus_reshape(data_corpus_inaugural, 'sentences')
ndoc(corpus_inaugural_sent)
corpus_inaugural_sent[1]
corpus_inaugural_sent[2]
?data_char_ukimmig2010
?tokens
nfeat(irish_dfm)
# dfm() constructs a document-feature matrix (DFM) from a tokens object
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
nfeat(irish_dfm)
irish_dfm[1]
irish_dfm[1,]
irish_dfm[,1]
irish_dfm[1,]
str(irish_dfm)
irish_dfm$features
irish_dfm
class(irish_dfm)
# identify most frequent features
topfeatures(irish_dfm, 10)
print(irish_dfm[1,])
as.matrix(irish_dfm[1,])
as.matrix(irish_dfm[,1])
features(irish_dfm)
irish_dfm$features
str(irish_dfm)
Dimnames(irish_dfm)
irish_dfm@Dimnames
irish_dfm@Dimnames[1:10]
irish_dfm@Dimnames[1:10,]
irish_dfm@Dimnames[1:10]
irish_dfm@Dimnames$features
irish_dfm@Dimnames$features[1:10]
# retrieve dictionary
newsmap_dict <- dictionary(file = "../data/newsmap.yml")
newsmap_dict
source("packages.r")
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors = F)
tweets$engaging <- ifelse(tweets$communication == "engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
#
set.seed(123)
sample(filter(tweets, engaging == 1), 10) %>% select(text)
filter(tweets, engaging == 1)
filter(tweets, engaging == 1) %>% sample_n(10) %>% select(text)
#
set.seed(123)
filter(tweets, engaging == 1) %>% sample_n(10) %>% select(text)
#
set.seed(123)
filter(tweets, engaging == 1) %>% sample_n(10) %>% select(text)
# inspect tweets
set.seed(123)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# inspect tweets
set.seed(123)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# inspect tweets
set.seed(42)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all('@[0-9_A-Za-z]+', '@', tweets$text)
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors = F)
tweets$engaging <- ifelse(tweets$communication == "engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
# inspect tweets
set.seed(42)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all('@[0-9_A-Za-z]+', '@', tweets$text)
tweets$text[1:10]
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors = F)
tweets$engaging <- ifelse(tweets$communication == "engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
# inspect tweets
set.seed(42)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all(tweets$text, '@[0-9_A-Za-z]+', '@')
tweets$text[1:10]
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
summary(twcorpus, 10)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out to be quite informative.
twdfm <- dfm(twcorpus, remove = stopwords("english"), remove_url = TRUE,
ngrams=1:2, verbose=TRUE)
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
# split into training and test set
set.seed(123)
floor(.80 * nrow(tweets))
nrow(tweets)
head(training)
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
head(training)
length(training)
length(test)
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
length(test)
914/nrow(tweets)
# ridge regression
parallel::detectCores()
?registerDoMC
registerDoMC(cores = 3) # parallel computing on multiple cores using the doMC package
ridge <- cv.glmnet(twdfm[training,], # x matrix
tweets$engaging[training],  # y response
family = "binomial", # family
alpha = 0,
nfolds = 5, # k-folds cross-validation
parallel = TRUE, # enable parallel computing
intercept = TRUE,
type.measure = "class")
plot(ridge)
?cv.glmnet
?glmnet
?cv.glmnet
plot.cv.glmnet()
?plot.cv.glmnet
?glmnet
plot(ridge)
?plot.cv.glmnet
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
plot(ridge)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
best.lambda
log(88)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
plot(lasso)
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
best.lambda
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ") # coefficients for some features actually became zero
