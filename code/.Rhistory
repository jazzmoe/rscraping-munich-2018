preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
# feature importance
labels <- dimnames(X)[[2]]
importance <- xgb.importance(labels, model = rf, data = X, label = tweets$engaging)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
head(importance, n=20)
# adding sign
sums <- list()
for (v in 0:1){
sums[[v+1]] <- colSums(X[tweets[,"engaging"]==v,])
}
sums <- do.call(cbind, sums)
sign <- apply(sums, 1, which.max)
df <- data.frame(
Feature = labels,
sign = sign-1,
stringsAsFactors=F)
importance <- merge(importance, df, by="Feature")
## best predictors
for (v in 0:1){
cat("\n\n")
cat("value==", v)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
print(head(importance[importance$sign==v,], n=50))
cat("\n")
cat(paste(unique(head(importance$Feature[importance$sign==v], n=50)), collapse=", "))
}
?xgboost
source("packages.r")
?xgboost
source("packages.r")
library(topicmodels)
## Topic modeling: LDA -------------
# dataset that contains the lead paragraph of around 5,000 articles about the economy published in the New York Times between 1980 and 2014. As before, we will preprocess the text using the standard set of techniques.
# The number of topics in a topic model is somewhat arbitrary, so you need to play with the number of topics to see if you get anything more meaningful. We start here with 30 topics.
# reading data and preparing corpus object
nyt <- read.csv("../data/nytimes.csv", stringsAsFactors = FALSE)
nytcorpus <- corpus(nyt$lead_paragraph)
nytdfm <- dfm(nytcorpus, remove=stopwords("english"), verbose=TRUE,
remove_punct=TRUE, remove_numbers=TRUE)
cdfm <- dfm_trim(nytdfm, min_docfreq = 2)
# estimate LDA with K topics
K <- 30
lda <- LDA(cdfm, k = K, method = "Gibbs",
control = list(verbose = 25L, seed = 123, burnin = 100, iter = 500))
# get top `n` terms from the topic model
terms <- get_terms(lda, 15)
terms[,1]
head(terms)
# predict the top `k` topic for each document.
topics <- get_topics(lda, 1)
head(topics)
# closer inspection of documents linked with topic
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 2)
# Topic 3
paste(terms[,3], collapse=", ")
sample(nyt$lead_paragraph[topics==3], 2)
# Topic 7
paste(terms[,7], collapse=", ")
sample(nyt$lead_paragraph[topics==7], 2)
# topics don't always make sense
# Topic 4
paste(terms[,4], collapse=", ")
sample(nyt$lead_paragraph[topics==4], 2)
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 1)
# add predicted topic to dataset
nyt$pred_topic <- topics
nyt$year <- substr(nyt$datetime, 1, 4) # extract year
# frequency table with articles about stock market, per year
tab <- table(nyt$year[nyt$pred_topic==2])
plot(tab)
# inspect mixture of topics (gamma matrix; theta on the slides)
round(lda@gamma[1,], 2)
# Topic 15: financial crisis
paste(terms[,15], collapse = ", ")
# add probability to df
nyt$prob_topic <- lda@gamma[,15]
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 2)
# Topic 15: financial crisis
paste(terms[,15], collapse = ", ")
# add probability to df
nyt$prob_topic <- lda@gamma[,15]
# now aggregate at the year level
agg <- aggregate(nyt$prob_topic, by=list(year=nyt$year), FUN=mean)
# and plot it
plot(agg$year, agg$x, type="l", xlab="Year", ylab="Avg. prob. of article about topic 15",
main="Estimated proportion of articles about the financial crisis")
## Structural topic todeling-------------
library(stm)
# extracting covariates
year <- as.numeric(substr(nyt$datetime, 1, 4))
repub <- ifelse(year %in% c(1981:1992, 2000:2008), 1, 0)
meta <- data.frame(year = year, repub = repub)
head(meta)
# load backup output
load("../data/stm-output.Rdata")
# looking at a few topics
labelTopics(stm, topics = 1)
labelTopics(stm, topics = 4)
labelTopics(stm, topics = 7)
labelTopics(stm, topics = 10)
?labelTopics
# estimate effects of features on prevalence of topics
est <- estimateEffect(~repub, stm, uncertainty = "None")
labelTopics(stm, topics = 1)
summary(est, topics = 1)
source("packages.r")
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
data.frame(name = name, phone = phone)
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_extract(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}")
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# self match
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# case sensitivity
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, ignore.case("SMALL")) # wrong
str_extract(example.obj, regex("SMALL", ignore_case = TRUE))
# match empty space
str_extract(example.obj, "mall sent")
# match the beginning of a string
str_extract(example.obj, "^1")
str_extract(example.obj, "^2")
# match the end of a string
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
# pipe operator
unlist(str_extract_all(example.obj, "tiny|sentence"))
# wildcard
str_extract(example.obj, "sm.ll")
# character class
str_extract(example.obj, "sm[abc]ll")
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# for more character classes, see
?base::regex
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:punct:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
# quantifier
str_extract(example.obj, "s[:alpha:][:alpha:][:alpha:]l")
str_extract(example.obj, "s[:alpha:]{3}l")
str_extract(example.obj, "A.+sentence")
# greedy quantification
str_extract(example.obj, "A.+sentence")
str_extract(example.obj, "A.+?sentence")
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
# meta characters
unlist(str_extract_all(example.obj, "\\."))
unlist(str_extract_all(example.obj, fixed(".")))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([:alpha:]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# replacement
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "a")
str_count(char.vec, "\\w+")
str_length(char.vec)
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
library(stringi)
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[munich]")
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//title")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
# 1. specify URL
url <- "https://www.nytimes.com"
browseURL(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
headings_nodes
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
headings
length(headings)
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
browseURL(url_p)
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
browseURL(url_p)
## scraping HTML tables with rvest
url <- "https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992"
browseURL(url)
url_p <- read_html(url)
?html_table
tables <- html_table(url_p, header = TRUE, fill = TRUE)
mps <- tables[[4]]
class(mps)
head(mps)
names(mps) <- c("constituency", "mp", "party")
mps <- mps[2:nrow(mps),]
head(mps)
View(mps)
str_subset(mps$constituency, "edit")
str_subset(mps$mp, "edit")
nrow(mps)
mps <- filter(mps, !str_detect(constituency, fixed("[edit]")))
nrow(mps)
View(mps)
nrow(mps)
View(mps)
table(mps$party, str_detect(mps$mp, "^Sir ")) # how many "Sirs" per party?
# to install it, visit
browseURL("http://selectorgadget.com/")
url <- "https://www.nytimes.com"
xpath <-  '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
xpath
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = xpath) %>% html_text()
# install current version of Java SE Runtime Environment
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jre10-downloads-4417026.html")
source("packages.r")
# install and load RSelenium (currently not on CRAN)
#devtools::install_github("johndharrison/binman")
#devtools::install_github("johndharrison/wdman")
#devtools::install_github("ropensci/RSelenium")
library(RSelenium)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
# open regions menu
xpath <- '/ul/li[1]/label'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
# open regions menu
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
# selection "European Union"
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
selectEU <- euElem$clickElement() # click on button
selectEU <- euElem$clickElement() # click on button
selectEU <- euElem$clickElement() # click on button
# set time frame
xpath <- '//*[@id="main"]/div/form/div[5]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="main"]/div/form/div[5]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
xpath <- '//*[@id="main"]/div/form/button[2]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
# close connection
remDr$closeServer()
# parse index table
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
# add names
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
View(tab)
## overview
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
# API documentation
browseURL("http://ip-api.com/")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
source("packages.r")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# ipapi package
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress = TRUE)
View(ip_df)
# register for key at
browseURL("http://developer.nytimes.com/")
# use API
library(rtimes)
install.packages("rtimes")
# use API
library(rtimes)
load("/Users/s.munzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
load("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
Sys.setenv(NYTIMES_AS_KEY = nytimes_apikey)
terms <- c("John McCain", "Nancy Pelosi", "Bernie Sanders", "Al Franken", "Marco Rubio", "Paul Ryan", "Elizabeth Warren", "Mitch McConnell", "Tim Kaine", "Dianne Feinstein")
nytimes_hits <- numeric()
for(i in seq_along(terms)) {
nytimes_hits[i] <-  as_search(q = terms[i], begin_date = "20170103", end_date = '20180601')$meta$hits
Sys.sleep(runif(1, 1, 2))
}
nytimes_hits_df <- data.frame(name = terms, nytimes_hits, stringsAsFactors = FALSE)
head(nytimes_hits_df)
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::(url)
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>%  as.data.frame(stringsAsFactors = FALSE)
ip_list %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
?fromJSON
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
ipapi_grabber("")
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
twitter_token
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
View(rt)
# some advice for search:
browseURL("https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators")
trump_bad <- search_tweets(URLencode("trump :("), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_good <- search_tweets(URLencode("trump :)"), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_good$text[1:5]
# check rate limits
rate_limits(token = twitter_token) %>% View()
# set keywords used to filter tweets
q <- paste0("merkel,trump,macron")
# parse directly into data frame
twitter_stream <- stream_tweets(q = q, timeout = 30, token = twitter_token)
View(twitter_stream)
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "politicians"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
#language = "de",
token = twitter_token)
# set up directory and JSON dump
rtweet.folder <- "../data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "politicians"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
#language = "de",
token = twitter_token)
# parse from json file
rt <- parse_stream(filename)
# inspect tweets data
names(rt)
# inspect users data
users_data(rt) %>% head()
user_df <- lookup_users("RDataCollection")
names(user_df)
View(user_df)
user_timeline_df <- get_timeline("RDataCollection")
user_favorites_df <- get_favorites("RDataCollection")
followers <- get_followers("RDataCollection")
friends <- get_friends("RDataCollection")
followers
