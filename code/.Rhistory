# dfm() constructs a document-feature matrix (DFM) from a tokens object
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
irish_dfm
# you can also pass corpus to dfm directly; remove_* arguments are passed to tokens() internally
irish_dfm <- dfm(data_corpus_irishbudget2010, remove_punct = TRUE)
irish_dfm
# dfm has many useful options
irish_dfm <- dfm(data_corpus_irishbudget2010,
tolower = TRUE,
remove_punct = TRUE,
stem = TRUE,
ngrams = 1:3 # passed on to tokens()
)
irish_dfm
irish_dfm <- dfm(data_corpus_irishbudget2010,
tolower = TRUE,
remove_punct = TRUE,
stem = TRUE,
ngrams = 1 # passed on to tokens()
)
irish_dfm
# dealing with stopwords
stopwords("english")
stopwords("german")
irish_dfm <- dfm(data_corpus_irishbudget2010,
remove_punct = TRUE,
remove=c(stopwords("english"), "£"),
verbose = TRUE)
# check number of documents, features, document names, feature names
ndoc(irish_dfm)
nfeat(irish_dfm)
docnames(irish_dfm)
head(featnames(irish_dfm), 20)
# calculate marginals of df matrix
rowSums(irish_dfm) %>% head(10)
colSums(irish_dfm) %>% head(10)
# identify most frequent features
topfeatures(irish_dfm, 10)
# convert frequency count to a proportion within documents
prop_irish_dfm <- dfm_weight(irish_dfm, scheme  = "prop")
topfeatures(prop_irish_dfm[1,])
# weight frequency count by uniqueness of the features ("term frequency-inverse document frequency", tf-idf)
tfidf_irish_dfm <- dfm_tfidf(irish_dfm)
tfidf_irish_dfm
head(tfidf_irish_dfm)
topfeatures(tfidf_irish_dfm)
?dfm_tfidf
# check top features in first document
topfeatures(tfidf_irish_dfm[1,])
# weight frequency count by uniqueness of the features ("term frequency-inverse document frequency", tf-idf); see https://en.wikipedia.org/wiki/Tf%E2%80%93idf
tfidf_irish_dfm <- dfm_tfidf(irish_dfm)
topfeatures(tfidf_irish_dfm[1,])
# convert frequency count to a proportion within documents
prop_irish_dfm <- dfm_weight(irish_dfm, scheme  = "prop")
topfeatures(prop_irish_dfm[1,])
# weight frequency count by uniqueness of the features ("term frequency-inverse document frequency", tf-idf); see https://en.wikipedia.org/wiki/Tf%E2%80%93idf
tfidf_irish_dfm <- dfm_tfidf(irish_dfm)
topfeatures(tfidf_irish_dfm[1,])
nfeat(irish_dfm)
nfeat(irish_dfm)
# remove stopwords
nostop_irish_dfm <- dfm_select(irish_dfm, stopwords('en'), selection = 'remove') # dfm_remove() works, too
nfeat(nostop_irish_dfm)
# minimum length of features
long_irish_dfm <- dfm_select(irish_dfm, min_nchar = 5)
nfeat(long_irish_dfm)
# minimum feature frequencies
freq_irish_dfm <- dfm_trim(irish_dfm, min_termfreq = 10)
nfeat(freq_irish_dfm)
# laver-garry.cat is a Wordstat dictionary that contain political left-right ideology keywords (Laver and Garry 2000)
lg_dict <- dictionary(file = "../data/laver-garry.cat")
?dfm_lookup
docvars(irish_dfm)
party_dfm <- dfm_group(irish_dfm, groups = docvars(irish_dfm, "party"))
ndoc(party_dfm)
docvars(party_dfm)
load("../data/tweetSample.RData")
names(tweetSample)
# generate corpus / document-feature matrix
tweet_corp <- corpus(tweetSample, text_field = "text")
tweet_dfm <- dfm(tweet_corp, remove_punct = TRUE, select = "#*") # only select hash tags
tweet_dfm
tweet_dfm[1:5]
head(tweet_dfm)
tweet_dfm[1:5,]
tweet_dfm[,1:5]
summary(tweet_dfm)
tweet_dfm[,1:5]
tweet_dfm[1:5,1:5]
tweet_corp
head(tweet_corp)
tweet_corp[1,]
tweet_corp[,1]
tweet_corp[1,]
tweet_corp[1:5,]
topfeatures(tweet_dfm)
tweet_dfm
topfeatures(tweet_dfm)
# identify top 10 frequent hashtags by group (language)
freq <- textstat_frequency(tweet_dfm, n = 10, groups = docvars(tweet_dfm, 'lang'))
head(freq, 20)
# dotplot of hashtag frequencies
tweet_dfm %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
# word cloud of 100 most common tags (DON'T do this)
textplot_wordcloud(tweet_dfm, max_words = 100)
# create document-level variable indicating whether Tweet was in English or other language
docvars(tweet_corp, "dummy_english") <- factor(ifelse(docvars(tweet_corp, "lang") == "English", "English", "Not English"))
tweet_corp_language <- dfm(tweet_corp, select = "#*", groups = "dummy_english")
# wordcloud with group comparison (again, DON'T do this)
textplot_wordcloud(tweet_corp_language, comparison = TRUE, max_words = 100)
inaug_toks <- tokens(data_corpus_inaugural)
inaug_dfm <- dfm(inaug_toks, remove = stopwords('en'))
lexdiv <- textstat_lexdiv(inaug_dfm)
tail(lexdiv, 5)
plot(lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(lexdiv)), labels = docvars(inaug_dfm, 'President'))
?textstat_lexdiv
?textstat_dist
?textstat_simil
dist <- textstat_dist(inaug_dfm)
as.matrix(dist)[1:10,1:10]
?textstat_dist
twdfm <- dfm(twcorpus, groups=c("screen_name"), remove_punct=TRUE,
remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target="realDonaldTrump",
measure="chi2"), n=20)
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)
## Exploring large-scale text datasets
tweets <- read.csv("../data/candidate-tweets.csv", stringsAsFactors = F)
# extract month data and subset only data during campaign
tweets$month <- substr(tweets$datetime, 1, 7)
tweets <- tweets[tweets$month>"2015-06",]
# create DFM at the candidate and month level
twcorpus <- corpus(tweets)
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)
docnames(twdfm)
textstat_simil(twdfm, margin="documents", method="jaccard")
textstat_simil(twdfm, margin="documents", method="cosine")
# generate corpus / document-feature matrix
tweets <- read.csv("../data/candidate-tweets.csv", stringsAsFactors = F)
# extract month data and subset only data during campaign
tweets$month <- substr(tweets$datetime, 1, 7)
tweets <- tweets[tweets$month>"2015-06",]
# create DFM at the candidate and month level
twcorpus <- corpus(tweets)
tweet_dfm <- dfm(twcorpus, remove_punct = TRUE, select = "#*") # only select hash tags
tweet_dfm
topfeatures(tweet_dfm)
# identify top 10 frequent hashtags by group (language)
freq <- textstat_frequency(tweet_dfm, n = 10, groups = docvars(tweet_dfm, 'lang'))
head(freq, 20)
summary(tweet_dfm)
names(tweet_dfm)
docvars(tweet_dfm)
docvars(tweet_dfm) %>% head()
# identify top 10 frequent hashtags
freq <- textstat_frequency(tweet_dfm, n = 10, groups = docvars(tweet_dfm, 'screen_name'))
head(freq, 20)
# identify top 3 frequent hashtags
freq <- textstat_frequency(tweet_dfm, n = 3, groups = docvars(tweet_dfm, 'screen_name'))
head(freq, 20)
head(freq, 30)
head(freq, 15)
# dotplot of hashtag frequencies
tweet_dfm %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
# word cloud of 100 most common tags (DON'T do this)
textplot_wordcloud(tweet_dfm, max_words = 100)
names(docvars(tweet_corp))
names(docvars(twcorpus))
table(docvars(twcorpus)$screen_name)
# create document-level variable indicating whether Tweet was in English or other language
docvars(twcorpus, "dummy_bernie") <- factor(ifelse(docvars(twcorpus, "screen_name") == "BernieSanders", "Bernie", "Not Bernie"))
tweet_corp_bernie <- dfm(twcorpus, select = "#*", groups = "dummy_bernie")
# wordcloud with group comparison (again: DON'T do this)
textplot_wordcloud(tweet_corp_bernie, comparison = TRUE, max_words = 100)
twdfm <- dfm(twcorpus, groups=c("screen_name"), remove_punct=TRUE,
remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target="realDonaldTrump",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="HillaryClinton",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="tedcruz",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="BernieSanders",
measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="realDonaldTrump",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="HillaryClinton",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="tedcruz",
measure="chi2"), n=5)
head(textstat_keyness(twdfm, target="BernieSanders",
measure="chi2"), n=5)
trump <- corpus_subset(twcorpus, screen_name=="realDonaldTrump")
twdfm <- dfm(trump, remove_punct=TRUE,
remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month<"2016-01",
measure="chi2"), n=5)
trump <- paste(
tweets$text[tweets$screen_name=="realDonaldTrump"], collapse=" ")
textplot_xray(kwic(trump, "hillary"), scale="relative")
textplot_xray(kwic(trump, "crooked"), scale="relative")
textplot_xray(kwic(trump, "mexic*"), scale="relative")
textplot_xray(kwic(trump, "fake"), scale="relative")
textplot_xray(kwic(trump, "immigr*"), scale="relative")
textplot_xray(kwic(trump, "immigr*"), scale="relative")
textplot_xray(kwic(trump, "muslim*"), scale="relative")
clinton <- paste(
tweets$text[tweets$screen_name=="HillaryClinton"], collapse=" ")
textplot_xray(kwic(clinton, "trump"), scale="relative")
textplot_xray(kwic(clinton, "sanders"), scale="relative")
textplot_xray(kwic(clinton, "gun*"), scale="relative")
load("../data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
# select tokens starting with capital letters
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
cap_col[1]
# identify and score multi-word expressions (with at least 10 observations)
cap_col <- textstat_collocations(cap_col, min_count = 10)
head(cap_col, 20)
# use results from analysis to compound meaningful (statistically strongly associated) tokens; this makes them less ambiguous and might improve statistical analyses
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
comp_toks[1]
# Lexicoder Sentiment Dictionary by Young/Soroka
browseURL("https://www.tandfonline.com/doi/abs/10.1080/10584609.2012.671234")
# data available in quanteda!
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
# load Guardian corpus
load("../data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
class(guardianSample)
guardianSample$tokens
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# check sentiment
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
# more targeted sentiment analysis
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks[2]
# grouped DFM
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
# plot absolute frequencies
matplot(eu_lsd_dfm, type = 'l', xaxt = 'n', lty = 1, ylab = 'Frequency')
grid()
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
legend('topright', col = 1:2, legend = c('Negative', 'Positive'), lty = 1, bg = 'white')
# plot average sentiment
eu_n <- ntoken(dfm(eu_toks, group = docvars(eu_toks, 'week')))
plot((eu_lsd_dfm[,2] - eu_lsd_dfm[,1]) / eu_n,
type = 'l', ylab = 'Sentiment', xlab = '', xaxt = 'n')
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
grid()
abline(h = 0, lty = 2)
# load data
uk <- read.csv("../data/FB-UK-parties.csv", stringsAsFactors = FALSE)
head(uk)
table(uk$party)
# build dictionary
populist_dict <- dictionary(list(
populism = c(
"elit*",
"consensus*",
"undemocratic*",
"referend*",
"corrupt*",
"propagand*",
"politici*",
"*deceit*",
"*deceiv*",
"*betray*",
"shame*",
"scandal*",
"truth*",
"dishonest*",
"establishm*",
"ruling*")))
# create corpus
fbcorpus <- corpus(uk)
fbdfm <- dfm(fbcorpus, groups = "party")
# normalize for document length: turn word counts into proportions
fbdfm <- dfm_weight(fbdfm, scheme="prop")
# find % of words in populism dictionary
pop <- dfm_lookup(fbdfm, dictionary = populist_dict)
pop * 100
# check precision
kwic(fbcorpus, pattern = 'undemocratic') # sounds good
kwic(fbcorpus, pattern = 'ruling*') # probably not
# check recall
kwic(fbcorpus, pattern = 'unaccountable')
kwic(fbcorpus, pattern = 'dodging')
?xgb.cv
?xgboost
## peparations -------------------
source("packages.r")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
?dgCMatrix
class(X)
# parameters to explore
tryEta <- c(.1, .3, .5)
tryDepths <- c(1, 3, 6)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
## regularized regression ----------
# random sample of nearly 5,000 tweets mentioning the names of the candidates to the 2014 EP elections in the UK.
# focus on variable named `communication`, which indicates whether each tweet was hand-coded as being __engaging__ (a tweet that tries to engage with the audience of the account) or __broadcasting__ (just sending a message, without trying to elicit a response)
# source: Yannis Theocharis, Pablo Barberá, Zoltan Fazekas, and Sebastian Popa,, Journal of Communication. (http://onlinelibrary.wiley.com/doi/10.1111/jcom.12259/abstract).
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors=F)
tweets$engaging <- ifelse(tweets$communication=="engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
# substitute handles with @ to avoid overfitting
tweets$text <- gsub('@[0-9_A-Za-z]+', '@', tweets$text)
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out it can be quite informative.
twdfm <- dfm(twcorpus, remove=stopwords("english"), remove_url=TRUE,
ngrams=1:2, verbose=TRUE) #
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
# parameters to explore
tryEta <- c(.1, .3, .5)
tryDepths <- c(3, 6)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
summary(data_corpus_inaugural)
source("packages.r")
summary(data_corpus_inaugural)
docvars(sotu_corpus)
docvars(data_corpus_inaugural)
docvars(data_corpus_inaugural) %>% head()
docvars(data_corpus_inaugural)$foo <- 1
docvars(data_corpus_inaugural) %>% head()
docvars(data_corpus_inaugural)$FullName <- paste(docvars(data_corpus_inaugural)$FirstName, docvars(data_corpus_inaugural)$President)
docvars(data_corpus_inaugural) %>% head()
summary(sotu_corpus, 5)
summary(data_corpus_inaugural, 5)
# subset corpus
str(data_corpus_inaugural) # structure of corpus object
# add metadata
docvars(data_corpus_inaugural) %>% head()
corpus_inaugural_sub <- corpus_subset(data_corpus_inaugural, Year >= 2000)
summary(corpus_inaugural_sub)
# change units of texts (documents, paragraphs, sentences)
ndoc(corpus_inaugural_sub)
# change units of texts (documents, paragraphs, sentences)
ndoc(data_corpus_inaugural)
corpus_inaugural_sent <- corpus_reshape(data_corpus_inaugural, 'sentences')
ndoc(corpus_inaugural_sent)
corpus_inaugural_sent[1]
corpus_inaugural_sent[2]
?data_char_ukimmig2010
?tokens
nfeat(irish_dfm)
# dfm() constructs a document-feature matrix (DFM) from a tokens object
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
nfeat(irish_dfm)
irish_dfm[1]
irish_dfm[1,]
irish_dfm[,1]
irish_dfm[1,]
str(irish_dfm)
irish_dfm$features
irish_dfm
class(irish_dfm)
# identify most frequent features
topfeatures(irish_dfm, 10)
print(irish_dfm[1,])
as.matrix(irish_dfm[1,])
as.matrix(irish_dfm[,1])
features(irish_dfm)
irish_dfm$features
str(irish_dfm)
Dimnames(irish_dfm)
irish_dfm@Dimnames
irish_dfm@Dimnames[1:10]
irish_dfm@Dimnames[1:10,]
irish_dfm@Dimnames[1:10]
irish_dfm@Dimnames$features
irish_dfm@Dimnames$features[1:10]
# retrieve dictionary
newsmap_dict <- dictionary(file = "../data/newsmap.yml")
newsmap_dict
