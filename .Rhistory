filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all('@[0-9_A-Za-z]+', '@', tweets$text)
tweets$text[1:10]
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors = F)
tweets$engaging <- ifelse(tweets$communication == "engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
# inspect tweets
set.seed(42)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all(tweets$text, '@[0-9_A-Za-z]+', '@')
tweets$text[1:10]
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus)
summary(twcorpus, 10)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out to be quite informative.
twdfm <- dfm(twcorpus, remove = stopwords("english"), remove_url = TRUE,
ngrams=1:2, verbose=TRUE)
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose=TRUE)
# split into training and test set
set.seed(123)
floor(.80 * nrow(tweets))
nrow(tweets)
head(training)
# split into training and test set
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
head(training)
length(training)
length(test)
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
length(test)
914/nrow(tweets)
# ridge regression
parallel::detectCores()
?registerDoMC
registerDoMC(cores = 3) # parallel computing on multiple cores using the doMC package
ridge <- cv.glmnet(twdfm[training,], # x matrix
tweets$engaging[training],  # y response
family = "binomial", # family
alpha = 0,
nfolds = 5, # k-folds cross-validation
parallel = TRUE, # enable parallel computing
intercept = TRUE,
type.measure = "class")
plot(ridge)
?cv.glmnet
?glmnet
?cv.glmnet
plot.cv.glmnet()
?plot.cv.glmnet
?glmnet
plot(ridge)
?plot.cv.glmnet
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
plot(ridge)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
best.lambda
log(88)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
plot(lasso)
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
best.lambda
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ") # coefficients for some features actually became zero
source("packages.r")
# data available in quanteda!
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
# load Guardian corpus
load("../data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# check sentiment: compare tokens with dictionary
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
# more targeted sentiment analysis
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks[2]
?tokens_keep
eu_toks
summary(eu_toks)
length(eu_toks)
length(guardian_toks)
eu_toks[2]
eu_toks[1]
eu_toks[2]
eu_toks[3]
# grouped DFM
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
# plot absolute frequencies
matplot(eu_lsd_dfm, type = 'l', xaxt = 'n', lty = 1, ylab = 'Frequency')
grid()
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
legend('topright', col = 1:2, legend = c('Negative', 'Positive'), lty = 1, bg = 'white')
# plot average sentiment
eu_n <- ntoken(dfm(eu_toks, group = docvars(eu_toks, 'week')))
plot((eu_lsd_dfm[,2] - eu_lsd_dfm[,1]) / eu_n,
type = 'l', ylab = 'Sentiment', xlab = '', xaxt = 'n')
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
grid()
abline(h = 0, lty = 2)
## peparations -------------------
source("packages.r")
## regularized regression ----------
# random sample of nearly 5,000 tweets mentioning the names of the candidates to the 2014 EP elections in the UK.
# source: Yannis Theocharis, Pablo BarberÃ¡, Zoltan Fazekas, and Sebastian Popa, Journal of Communication. (http://onlinelibrary.wiley.com/doi/10.1111/jcom.12259/abstract).
# focus on variable named `communication`, which indicates whether each tweet was hand-coded as being
# engaging (a tweet that tries to engage with the audience of the account) or
# broadcasting (just sending a message, without trying to elicit a response)
# import data
tweets <- read.csv("../data/UK-tweets.csv", stringsAsFactors = F)
tweets$engaging <- ifelse(tweets$communication == "engaging", 1, 0)
tweets <- tweets[!is.na(tweets$engaging),]
head(tweets)
# inspect tweets
set.seed(42)
filter(tweets, engaging == 1) %>% sample_n(5) %>% select(text)
filter(tweets, engaging == 0) %>% sample_n(5) %>% select(text)
# substitute handles with @ to avoid overfitting
tweets$text <- str_replace_all(tweets$text, '@[0-9_A-Za-z]+', '@')
tweets$text[1:10]
# further preprocessing
twcorpus <- corpus(tweets$text)
summary(twcorpus, 10)
# keep only tokens that appear in 2 or more tweets
# keep punctuation -- it turns out to be quite informative.
twdfm <- dfm(twcorpus, remove = stopwords("english"), remove_url = TRUE,
ngrams = 1:2, verbose = TRUE)
twdfm <- dfm_trim(twdfm, min_docfreq = 2, verbose = TRUE)
# split into training and test set (80% / 20%)
set.seed(123)
training <- sample(1:nrow(tweets), floor(.80 * nrow(tweets)))
test <- (1:nrow(tweets))[1:nrow(tweets) %in% training == FALSE]
# ridge regression
parallel::detectCores()
registerDoMC(cores = 3) # parallel computing on multiple cores using the doMC package
ridge <- cv.glmnet(twdfm[training,], # x matrix
tweets$engaging[training],  # y response
family = "binomial", # family; here: dichotomous response
alpha = 0, # elastic net mixing parameter; alpha = 0 --> ridge, alpha = 1 --> lasso
nfolds = 5, # k-folds cross-validation
parallel = TRUE, # enable parallel computing
intercept = TRUE, # intecept to be fitted?
type.measure = "class")
plot(ridge)
summary(ridge)
summary(ridge$glmnet.fit)
dim(twdfm[training,])
nrow(training)
class(training)
length(training)
length(test)
nrow(twdfm[training,])
ncol(twdfm[training,]) # number of features
ncol(twdfm[test,]) # number of features
## function to compute accuracy
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, twdfm[test,], type="class")
table(preds, tweets$engaging[test]) # confusion matrix
# performance metrics
accuracy(preds, tweets$engaging[test])
precision(preds == 1, tweets$engaging[test] == 1)
recall(preds == 1, tweets$engaging[test] == 1)
precision(preds == 0, tweets$engaging[test] == 0)
17/20
recall(preds == 0, tweets$engaging[test] == 0)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
best.lambda
log(best.lambda)
# from the different values of lambda, let's pick the highest one that is within one standard error of the best one
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# lasso regression
lasso <- cv.glmnet(twdfm[training,],
tweets$engaging[training],
family = "binomial",
alpha = 1, # <- here's the difference
nfolds = 5,
parallel = TRUE,
intercept = TRUE,
type.measure = "class")
# computing predicted values
preds <- predict(lasso, twdfm[test,], type="class")
# confusion matrix
table(preds, tweets$engaging[test])
# performance metrics (slightly better!)
accuracy(preds, tweets$engaging[test])
precision(preds==1, tweets$engaging[test]==1)
recall(preds==1, tweets$engaging[test]==1)
precision(preds==0, tweets$engaging[test]==0)
recall(preds==0, tweets$engaging[test]==0)
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors = F)
df <- df[order(df$coef, decreasing = TRUE),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ") # coefficients for some features actually became zero
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n = 30)
paste(df$word[1:30], collapse = ", ")
# converting matrix object
X <- as(twdfm, "dgCMatrix")
# parameters to explore
tryEta <- c(.1, .3, .5)
tryDepths <- c(3, 6)
# placeholders for now
bestEta = NA
bestDepth = NA
bestAcc = 0
for(eta in tryEta){
for(dp in tryDepths){
bst <- xgb.cv(data = X[training,],
label =  tweets$engaging[training],
max.depth = dp,
eta = eta,
nthread = 4,
nround = 500,
nfold = 5,
print_every_n = 100L,
objective = "binary:logistic")
# cross-validated accuracy
acc <- 1-mean(tail(bst$evaluation_log$test_error_mean))
cat("Results for eta=",eta," and depth=", dp, " : ",
acc," accuracy.\n",sep="")
if(acc>bestAcc){
bestEta=eta
bestAcc=acc
bestDepth=dp
}
}
}
cat("Best model has eta=",bestEta," and depth=", bestDepth, " : ",
bestAcc," accuracy.\n",sep="")
# running best model
rf <- xgboost(data = X[training,],
label = tweets$engaging[training],
max.depth = bestDepth,
eta = bestEta,
nthread = 4,
nround = 1000,
print_every_n=100L,
objective = "binary:logistic")
# out-of-sample accuracy
preds <- predict(rf, X[test,])
cat("\nAccuracy on test set=", round(accuracy(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(1) on test set=", round(precision(preds>.50, tweets$engaging[test]),3))
cat("\nRecall(1) on test set=", round(recall(preds>.50, tweets$engaging[test]),3))
cat("\nPrecision(0) on test set=", round(precision(preds<.50, tweets$engaging[test]==0),3))
cat("\nRecall(0) on test set=", round(recall(preds<.50, tweets$engaging[test]==0),3))
# feature importance
labels <- dimnames(X)[[2]]
importance <- xgb.importance(labels, model = rf, data = X, label = tweets$engaging)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
head(importance, n=20)
# adding sign
sums <- list()
for (v in 0:1){
sums[[v+1]] <- colSums(X[tweets[,"engaging"]==v,])
}
sums <- do.call(cbind, sums)
sign <- apply(sums, 1, which.max)
df <- data.frame(
Feature = labels,
sign = sign-1,
stringsAsFactors=F)
importance <- merge(importance, df, by="Feature")
## best predictors
for (v in 0:1){
cat("\n\n")
cat("value==", v)
importance <- importance[order(importance$Gain, decreasing=TRUE),]
print(head(importance[importance$sign==v,], n=50))
cat("\n")
cat(paste(unique(head(importance$Feature[importance$sign==v], n=50)), collapse=", "))
}
?xgboost
source("packages.r")
?xgboost
source("packages.r")
library(topicmodels)
## Topic modeling: LDA -------------
# dataset that contains the lead paragraph of around 5,000 articles about the economy published in the New York Times between 1980 and 2014. As before, we will preprocess the text using the standard set of techniques.
# The number of topics in a topic model is somewhat arbitrary, so you need to play with the number of topics to see if you get anything more meaningful. We start here with 30 topics.
# reading data and preparing corpus object
nyt <- read.csv("../data/nytimes.csv", stringsAsFactors = FALSE)
nytcorpus <- corpus(nyt$lead_paragraph)
nytdfm <- dfm(nytcorpus, remove=stopwords("english"), verbose=TRUE,
remove_punct=TRUE, remove_numbers=TRUE)
cdfm <- dfm_trim(nytdfm, min_docfreq = 2)
# estimate LDA with K topics
K <- 30
lda <- LDA(cdfm, k = K, method = "Gibbs",
control = list(verbose = 25L, seed = 123, burnin = 100, iter = 500))
# get top `n` terms from the topic model
terms <- get_terms(lda, 15)
terms[,1]
head(terms)
# predict the top `k` topic for each document.
topics <- get_topics(lda, 1)
head(topics)
# closer inspection of documents linked with topic
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 2)
# Topic 3
paste(terms[,3], collapse=", ")
sample(nyt$lead_paragraph[topics==3], 2)
# Topic 7
paste(terms[,7], collapse=", ")
sample(nyt$lead_paragraph[topics==7], 2)
# topics don't always make sense
# Topic 4
paste(terms[,4], collapse=", ")
sample(nyt$lead_paragraph[topics==4], 2)
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 1)
# add predicted topic to dataset
nyt$pred_topic <- topics
nyt$year <- substr(nyt$datetime, 1, 4) # extract year
# frequency table with articles about stock market, per year
tab <- table(nyt$year[nyt$pred_topic==2])
plot(tab)
# inspect mixture of topics (gamma matrix; theta on the slides)
round(lda@gamma[1,], 2)
# Topic 15: financial crisis
paste(terms[,15], collapse = ", ")
# add probability to df
nyt$prob_topic <- lda@gamma[,15]
# Topic 2
paste(terms[,2], collapse=", ")
sample(nyt$lead_paragraph[topics==2], 2)
# Topic 15: financial crisis
paste(terms[,15], collapse = ", ")
# add probability to df
nyt$prob_topic <- lda@gamma[,15]
# now aggregate at the year level
agg <- aggregate(nyt$prob_topic, by=list(year=nyt$year), FUN=mean)
# and plot it
plot(agg$year, agg$x, type="l", xlab="Year", ylab="Avg. prob. of article about topic 15",
main="Estimated proportion of articles about the financial crisis")
## Structural topic todeling-------------
library(stm)
# extracting covariates
year <- as.numeric(substr(nyt$datetime, 1, 4))
repub <- ifelse(year %in% c(1981:1992, 2000:2008), 1, 0)
meta <- data.frame(year = year, repub = repub)
head(meta)
# load backup output
load("../data/stm-output.Rdata")
# looking at a few topics
labelTopics(stm, topics = 1)
labelTopics(stm, topics = 4)
labelTopics(stm, topics = 7)
labelTopics(stm, topics = 10)
?labelTopics
# estimate effects of features on prevalence of topics
est <- estimateEffect(~repub, stm, uncertainty = "None")
labelTopics(stm, topics = 1)
summary(est, topics = 1)
## preparations -----------------------
source("packages.r")
browseURL("http://www.biermap24.de/brauereiliste.php")
## preparations -----------------------
source("packages.r")
browseURL("http://www.biermap24.de/brauereiliste.php")
# set temporary working directory
tempwd <- ("../data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
anchors
cities <- html_text(anchors)
cities
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
length(cities)
length(unique(cities))
sort(table(cities))
# geocoding takes a while -> save results in local cache file
# 2500 requests allowed per day
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=.8)
p
setwd("../../")
# set temporary working directory
tempwd <- ("../data/wikipediaStatisticians")
dir.create(tempwd)
