example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_extract(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}")
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# self match
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# case sensitivity
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, ignore.case("SMALL")) # wrong
str_extract(example.obj, regex("SMALL", ignore_case = TRUE))
# match empty space
str_extract(example.obj, "mall sent")
# match the beginning of a string
str_extract(example.obj, "^1")
str_extract(example.obj, "^2")
# match the end of a string
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
# pipe operator
unlist(str_extract_all(example.obj, "tiny|sentence"))
# wildcard
str_extract(example.obj, "sm.ll")
# character class
str_extract(example.obj, "sm[abc]ll")
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# for more character classes, see
?base::regex
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:punct:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
# quantifier
str_extract(example.obj, "s[:alpha:][:alpha:][:alpha:]l")
str_extract(example.obj, "s[:alpha:]{3}l")
str_extract(example.obj, "A.+sentence")
# greedy quantification
str_extract(example.obj, "A.+sentence")
str_extract(example.obj, "A.+?sentence")
# quantifier with pattern sequence
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
# meta characters
unlist(str_extract_all(example.obj, "\\."))
unlist(str_extract_all(example.obj, fixed(".")))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([:alpha:]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# replacement
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "a")
str_count(char.vec, "\\w+")
str_length(char.vec)
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
library(stringi)
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[munich]")
source("packages.r")
# parse with read_html
parsed_doc <- read_html("https://google.com")
parsed_doc
# inspect parsed object
class(parsed_doc)
html_structure(parsed_doc)
as_list(parsed_doc)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//title")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
# 1. specify URL
url <- "https://www.nytimes.com"
browseURL(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
headings_nodes <- html_nodes(url_parsed, xpath = "//*[@class = 'story-heading']")
headings_nodes
# 4. extract content from nodes
headings <- html_text(headings_nodes)
headings
# 5. tidy headlines
headings <- str_replace_all(headings, "\\n|\\t|\\r", " ") %>% str_trim()
head(headings)
headings
length(headings)
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
browseURL(url_p)
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
browseURL(url_p)
## scraping HTML tables with rvest
url <- "https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992"
browseURL(url)
url_p <- read_html(url)
?html_table
tables <- html_table(url_p, header = TRUE, fill = TRUE)
mps <- tables[[4]]
class(mps)
head(mps)
names(mps) <- c("constituency", "mp", "party")
mps <- mps[2:nrow(mps),]
head(mps)
View(mps)
str_subset(mps$constituency, "edit")
str_subset(mps$mp, "edit")
nrow(mps)
mps <- filter(mps, !str_detect(constituency, fixed("[edit]")))
nrow(mps)
View(mps)
nrow(mps)
View(mps)
table(mps$party, str_detect(mps$mp, "^Sir ")) # how many "Sirs" per party?
# to install it, visit
browseURL("http://selectorgadget.com/")
url <- "https://www.nytimes.com"
xpath <-  '//*[contains(concat( " ", @class, " " ), concat( " ", "story-heading", " " ))]//a'
xpath
url_parsed <- read_html(url)
html_nodes(url_parsed, xpath = xpath) %>% html_text()
# install current version of Java SE Runtime Environment
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jre10-downloads-4417026.html")
source("packages.r")
# install and load RSelenium (currently not on CRAN)
#devtools::install_github("johndharrison/binman")
#devtools::install_github("johndharrison/wdman")
#devtools::install_github("ropensci/RSelenium")
library(RSelenium)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
# open regions menu
xpath <- '/ul/li[1]/label'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
# open regions menu
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
# selection "European Union"
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
selectEU <- euElem$clickElement() # click on button
selectEU <- euElem$clickElement() # click on button
selectEU <- euElem$clickElement() # click on button
# set time frame
xpath <- '//*[@id="main"]/div/form/div[5]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="main"]/div/form/div[5]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
xpath <- '//*[@id="main"]/div/form/button[2]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
# close connection
remDr$closeServer()
# parse index table
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
# add names
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
View(tab)
## overview
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
# API documentation
browseURL("http://ip-api.com/")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
source("packages.r")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# ipapi package
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress = TRUE)
View(ip_df)
# register for key at
browseURL("http://developer.nytimes.com/")
# use API
library(rtimes)
install.packages("rtimes")
# use API
library(rtimes)
load("/Users/s.munzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
load("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
Sys.setenv(NYTIMES_AS_KEY = nytimes_apikey)
terms <- c("John McCain", "Nancy Pelosi", "Bernie Sanders", "Al Franken", "Marco Rubio", "Paul Ryan", "Elizabeth Warren", "Mitch McConnell", "Tim Kaine", "Dianne Feinstein")
nytimes_hits <- numeric()
for(i in seq_along(terms)) {
nytimes_hits[i] <-  as_search(q = terms[i], begin_date = "20170103", end_date = '20180601')$meta$hits
Sys.sleep(runif(1, 1, 2))
}
nytimes_hits_df <- data.frame(name = terms, nytimes_hits, stringsAsFactors = FALSE)
head(nytimes_hits_df)
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::(url)
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>%  as.data.frame(stringsAsFactors = FALSE)
ip_list %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
?fromJSON
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
ipapi_grabber("")
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
twitter_token
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
View(rt)
# some advice for search:
browseURL("https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators")
trump_bad <- search_tweets(URLencode("trump :("), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_good <- search_tweets(URLencode("trump :)"), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_good$text[1:5]
# check rate limits
rate_limits(token = twitter_token) %>% View()
# set keywords used to filter tweets
q <- paste0("merkel,trump,macron")
# parse directly into data frame
twitter_stream <- stream_tweets(q = q, timeout = 30, token = twitter_token)
View(twitter_stream)
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "politicians"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
#language = "de",
token = twitter_token)
# set up directory and JSON dump
rtweet.folder <- "../data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "politicians"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
#language = "de",
token = twitter_token)
# parse from json file
rt <- parse_stream(filename)
# inspect tweets data
names(rt)
# inspect users data
users_data(rt) %>% head()
user_df <- lookup_users("RDataCollection")
names(user_df)
View(user_df)
user_timeline_df <- get_timeline("RDataCollection")
user_favorites_df <- get_favorites("RDataCollection")
followers <- get_followers("RDataCollection")
friends <- get_friends("RDataCollection")
followers
## peparations -------------------
source("packages.r")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
setwd("/Users/simonmunzert/GitHub/rscraping-munich-2018/")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
content(url_out) %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
library(rvest)
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/rscraping-munich-2018/")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out) %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
?content
httr::content(url_out) %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
httr::content(url_out)
httr::content(url_out) %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
httr::content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl start spiegelheadlines")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/rscraping-munich-2018/")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
httr::content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
load("/Users/simonmunzert/GitHub/rscraping-munich-2018/code/news_scraper/nytimes_df_2018_june.RDa")
View(nytimes_df)
