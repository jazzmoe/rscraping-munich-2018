textstat_collocations(toks, min_count = 50)
textstat_collocations(toks, min_count = 20)
textstat_collocations(nostop_toks, min_count = 20)
textstat_collocations(nostop_toks, min_count = 10)
textstat_collocations(nostop_toks, min_count = 5)
## peparations -------------------
source("packages.r")
newsmap_dict <- dictionary(file = "content/dictionary/newsmap.yml")
newsmap_dict <- dictionary(file = "data/newsmap.yml")
View(newsmap_dict)
names(newsmap_dict)
newsmap_dict[['AFRICA']][['NORTH']]
# words
immig_corp <- corpus(data_char_ukimmig2010)
toks <- tokens(immig_corp, what = "word")
summary(toks)
region_toks <- tokens_lookup(toks, newsmap_dict, levels = 1)
head(region_toks)
# look up
region_toks <- tokens_lookup(toks, newsmap_dict, levels = 2)
head(region_toks)
# retrieve dictionary
newsmap_dict <- dictionary(file = "data/newsmap.yml")
names(newsmap_dict)
names(newsmap_dict[['AFRICA']])
newsmap_dict[['AFRICA']][['NORTH']]
# make document-feature matrix
dfm(region_toks)
# look up
region_toks <- tokens_lookup(toks, newsmap_dict, levels = 1)
head(region_toks)
# look up countries
country_toks <- tokens_lookup(toks, newsmap_dict, levels = 3)
head(country_toks)
newsmap_dict[['EUROPE']][['NORTH']] # level 3
dfm(country_toks)
# keywords in context
kwic(toks, newsmap_dict['AFRICA'])
# keywords in context
kwic(toks, newsmap_dict['AFRICA'], window = 10)
# create your own dictionary
dict <- dictionary(list(refugee = c('refugee*', 'asylum*'),
worker = c('worker*', 'employee*')))
print(dict)
dict_toks <- tokens_lookup(toks, dict)
head(dict_toks)
ngram <- tokens_ngrams(toks, n = 2:4)
head(ngram[[1]], 50)
length(ngram)
length(ngram[[1]])
length(toks[[1]])
toks[[1]]
?token_ngrams
?tokens_ngrams
# generate skip-grams
skipgram <- tokens_ngrams(toks, n = 2, skip = 1:2)
head(skipgram[[1]], 50)
skipgram[[1]]
# generate skip-grams
skipgram <- tokens_ngrams(toks, n = 2, skip = 1:2)
head(skipgram[[1]], 50)
kwic(skipgram[[1]][41], window = 10)
skipgram[[1]][41]
kwic(skipgram[[1]], window = 10)[1:10]
kwic(skipgram, window = 10)[1:10]
?kwic
kwic(skipgram, "indigenous_people", window = 10)
phrase
?phrase
comp_toks <- tokens_compound(toks, phrase('not *'))
neg_bigram <- tokens_compound(toks, phrase('not *'))
neg_bigram
neg_bigram <- tokens_select(neg_bigram, phrase('not_*'))
neg_bigram
head(neg_bigram[[1]], 50)
# how to discover popular multi-words expressions in the first place?
textstat_collocations(toks, min_count = 5)
# how to discover popular multi-words expressions in the first place?
textstat_collocations(nostop_toks, min_count = 5)
# explicit remove function
nostop_toks <- tokens_remove(toks, stopwords('en'))
# how to discover popular multi-words expressions in the first place?
textstat_collocations(nostop_toks, min_count = 5)
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
irish_dfm <- dfm(irish_toks)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
summary(irish_dfm)
irish_dfm
irish_dfm <- dfm(data_corpus_irishbudget2010)
irish_dfm
irish_dfm <- dfm(data_corpus_irishbudget2010, remove_punct = TRUE)
irish_dfm
ndoc(irish_dfm)
nfeat(irish_dfm)
docnames(irish_dfm)
featnames(irish_dfm)
featnames(irish_dfm, 20)
head(featnames(irish_dfm), 20)
class(irish_dfm)
# calculate marginals of df matrix
rowSums(irish_dfm) %>% head(10)
colSums(irish_dfm) %>% head(10)
topfeatures
# identify most frequent features
topfeatures(irish_dfm, 10)
# convert frequency count to a proportion within documents
prop_irish_dfm <- dfm_weight(irish_dfm, scheme  = "prop")
topfeatures(prop_irish_dfm[1,])
?dfm_weight
?dfm_tfidf
# weight frequency count by uniqueness of the features ("term frequency-inverse document frequency", tf-idf)
tfidf_irish_dfm <- dfm_tfidf(irish_dfm)
# check topfeatures in first document
topfeatures(tfidf_irish_dfm[1,])
source("packages.r")
## construct a document-feature matrix (DFM) ---------------------
# dfm() constructs a document-feature matrix (DFM) from a tokens object
irish_toks <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
summary(irish_toks)
irish_dfm <- dfm(irish_toks)
irish_dfm
# you can also pass corpus to dfm directly; remove_* arguments are passed to tokens() internally
irish_dfm <- dfm(data_corpus_irishbudget2010, remove_punct = TRUE)
irish_dfm
nfeat(irish_dfm)
nostop_irish_dfm <- dfm_select(irish_dfm, stopwords('en'), selection = 'remove')
nfeat(nostop_irish_dfm)
# minimum length of features
long_irish_dfm <- dfm_select(irish_dfm, min_nchar = 5)
nfeat(long_irish_dfm)
# minimum feature frequencies
freq_irish_dfm <- dfm_trim(irish_dfm, min_termfreq = 10)
nfeat(long_irish_dfm)
nfeat(freq_irish_dfm)
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, max_docfreq = 0.5)
nfeat(docfreq_irish_dfm)
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, max_docfreq = 0.75)
nfeat(docfreq_irish_dfm)
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, max_docfreq = 0.95)
nfeat(docfreq_irish_dfm)
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, max_docfreq = 0.15)
nfeat(docfreq_irish_dfm)
?dfm_trim
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, min_docfreq = 0.15)
nfeat(docfreq_irish_dfm)
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, min_docfreq = 0.95)
nfeat(docfreq_irish_dfm)
docfreq_irish_dfm
# maximum number of appearances in documents (here: features that occur in more than half of the documents are removed)
docfreq_irish_dfm <- dfm_trim(irish_dfm, min_docfreq = 0.15)
nfeat(docfreq_irish_dfm)
# laver-garry.cat is a Wordstat dictionary that contain political left-right ideology keywords (Laver and Garry 2000)
lg_dict <- dictionary(file = "data/laver-garry.cat")
lg_dfm <- dfm_lookup(irish_dfm, lg_dict)
head(lg_dfm)
summary(irish_dfm)
irish_dfm
head(irish_dfm, 5)
docvars(irish_dfm)
party_dfm <- dfm_group(irish_dfm, groups = docvars(irish_dfm, "party"))
ndoc(party_dfm)
docvars(party_dfm)
## peparations -------------------
source("packages.r")
## simple frequency analysis ---------------------
load("data/tweetSample.RData")
View(tweetSample)
?corpus
names(tweetSample)
corpus(tweetSample, text_field = "text")
tweet_corp <- corpus(tweetSample, text_field = "text")
tweet_toks <- tokens(tweet_corp, remove_punct = TRUE)
tweet_dfm <- dfm(tweet_toks, select = "#*")
freq <- textstat_frequency(tweet_dfm, n = 5, groups = docvars(tweet_dfm, 'lang'))
head(freq, 20)
tweet_dfm <- dfm(tweet_corp, remove_punct = TRUE, select = "#*")
s
tweet_dfm
summary(tweet_dfm)
tweet_dfm <- dfm(tweet_corp, remove_punct = TRUE, select = "#*") # only select hash tags
tweet_dfm
?textstat_frequency
freq <- textstat_frequency(tweet_dfm, n = 5, groups = docvars(tweet_dfm, 'lang'))
head(freq, 20)
freq <- textstat_frequency(tweet_dfm, n = 10, groups = docvars(tweet_dfm, 'lang'))
head(freq, 20)
# plot hashtag frequencies
tweet_dfm %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
# plot hashtag frequencies
tweet_dfm %>%
textstat_frequency() %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
# plot hashtag frequencies
tweet_dfm %>%
textstat_frequency(n = 15) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
# word cloud of 100 most common tags (don't do this)
textplot_wordcloud(tweet_dfm, max_words = 100)
# create document-level variable indicating whether Tweet was in English or other language
docvars(tweet_corp, "dummy_english") <- factor(ifelse(docvars(tweet_corp, "lang") == "English", "English", "Not English"))
tweet_corp_language <- dfm(tweet_corp, select = "#*", groups = "dummy_english")
textplot_wordcloud(tweet_corp_language, comparison = TRUE, max_words = 200)
# wordcloud with group comparison (again, DON'T do this)
textplot_wordcloud(tweet_corp_language, comparison = TRUE, max_words = 100)
inaug_toks <- tokens(data_corpus_inaugural)
inaug_dfm <- dfm(inaug_toks, remove = stopwords('en'))
lexdiv <- textstat_lexdiv(inaug_dfm)
tail(lexdiv, 5)
plot(lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(lexdiv)), labels = docvars(inaug_dfm, 'President'))
?textstat_dist
dist <- textstat_dist(inaug_dfm)
dist
summary(dist)
dist[1:10,1:10]
dim(dist)
class(dist)
dist
dist[1]
dist[1:10]
as.matrix(dist)[1:10,1:10]
?hclust
clust <- hclust(dist)
plot(clust, xlab = "Distance", ylab = NULL)
news_toks <- tokens(data_corpus_inaugural, remove_punct = TRUE)
cap_col <- tokens_select(news_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 100)
head(cap_col, 20)
cap_col <- tokens_select(news_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 50)
head(cap_col, 20)
cap_col <- tokens_select(news_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 10)
head(cap_col, 20)
load("data/guardianSample.RData")
names(guardianSample)
guardianSample$documents
guardian_corp <- corpus(guardianSample$documents)
names(guardianSample)
guardian_corp <- corpus(guardianSample$tokens)
class(guardianSample$documents)
names(guardianSample$documents)
?corpus
guardian_corp <- corpus(guardianSample$tokens, text_field = "texts")
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 10)
head(cap_col, 20)
?textstat_collocations
# select tokens starting with capital letters;
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 10)
head(cap_col, 20)
?tokens_compound
cap_col[cap_col$z > 3]
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
guardian_toks[['text7005']][370:450] # before compounding
comp_toks[['text7005']][370:450] # before compounding
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
guardian_toks[['text7005']][370:450] # before compounding
comp_toks[['text7005']][370:450] # before compounding
names(guardian_toks)
guardian_toks[['text7005']][370:450] # before compounding
head(cap_toks[[1]], 50)
comp_toks[[1]]
comp_toks[[2]]
comp_toks[[3]]
comp_toks[[4]]
comp_toks[[5]]
comp_toks[[6]]
class(comp_toks)
# identify and score multi-word expressions (with at least 10 observations)
cap_col %>% textstat_collocations(min_count = 10)
# select tokens starting with capital letters
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
# identify and score multi-word expressions (with at least 10 observations)
cap_col %>% textstat_collocations(min_count = 10)
head(cap_col, 20)
# identify and score multi-word expressions (with at least 10 observations)
cap_col <- textstat_collocations(cap_col, min_count = 10)
head(cap_col, 20)
cameron_toks <- tokens_select(toks, c('david cameron'))
cameron_toks <- tokens_select(guardian_toks, c('david cameron'))
cameron_toks
cameron_toks[[1]]
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
class(guardian_toks)
dim(guardian_toks)
length(guardian_toks)
guardian_toks[1]
# select tokens starting with capital letters
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
cap_col[1]
# identify and score multi-word expressions (with at least 10 observations)
cap_col <- textstat_collocations(cap_col, min_count = 10)
head(cap_col, 20)
cap_col[cap_col$z > 3]
# use results from analysis to compound meaningful (statistically strongly associated) tokens; this makes them less ambiguous and might improve statistical analyses
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
comp_toks
comp_toks[1]
comp_toks[5]
comp_toks[4]
lengths(data_dictionary_LSD2015)
summary(guardian_toks)
docvars(guardian_toks) %>% head()
docvars(guardian_corp) %>% head()
docvars(guardian_corp)$date[1]
docvars(guardian_corp)$date[1] %>% class
guardian_toks <- corpus_subset(guardian_corp, date >= as.Date("2016-01-01"))
guardian_corp <- corpus_subset(guardian_corp, date >= as.Date("2016-01-01"))
load("data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_corp <- corpus_subset(guardian_corp, date >= as.Date("2016-06-01"))
docvars(guardian_corp) %>% head()
lsd_toks <- tokens_lookup(guardian_corp, data_dictionary_LSD2015[1:2])
# subset corpus
guardian_corp <- corpus_subset(guardian_corp, date >= as.Date("2016-06-01"))
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
?lengths
data_dictionary_LSD2015[1]
data_dictionary_LSD2015[1][1:10]
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
data_dictionary_LSD2015[1]
class(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])
as.list(data_dictionary_LSD2015[1])[1]
as.list(data_dictionary_LSD2015[1])$negative[1:10]
as.list(data_dictionary_LSD2015[1])$negative[1:50]
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[2])$negative[1:50]
as.list(data_dictionary_LSD2015[2])$positive[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(news_toks, phrase(eu), window = 10)
eu_toks
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks
eu_toks[1]
eu_toks[2]
eu_toks[3]
eu_toks[4]
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
load("data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
docvars(guardian_corp)
head(docvars(guardian_corp))
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
class(guardian_corp)
## peparations -------------------
source("packages.r")
## collocation analysis ----------
load("data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
# subset corpus
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
docvars(guardian_corp) %>% head()
guardian_corp <- corpus_subset(guardian_corp, year >= 2016, month >= 7)
docvars(guardian_corp) %>% head()
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# check sentiment
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
# more targeted sentiment analysis
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks[4]
eu_toks[1]
eu_toks[2]
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
docvars(eu_toks)[1]
?docvars
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
source("packages.r")
load("data/guardianSample.RData")
names(guardianSample)
guardian_corp <- corpus(guardianSample$documents, text_field = "texts")
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
guardian_toks[1]
# select tokens starting with capital letters
cap_col <- tokens_select(guardian_toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
cap_col[1]
# identify and score multi-word expressions (with at least 10 observations)
cap_col <- textstat_collocations(cap_col, min_count = 10)
head(cap_col, 20)
# use results from analysis to compound meaningful (statistically strongly associated) tokens; this makes them less ambiguous and might improve statistical analyses
comp_toks <- tokens_compound(guardian_toks, cap_col[cap_col$z > 3])
comp_toks[1]
# Lexicoder Sentiment Dictionary by Young/Soroka
lengths(data_dictionary_LSD2015)
as.list(data_dictionary_LSD2015[1])$negative[1:50]
as.list(data_dictionary_LSD2015[3])$neg_positive[1:50]
docvars(guardian_corp) %>% head()
class(guardian_corp)
week(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
docvars(guardian_corp) %>% head
docvars(guardian_corp, 'year') <- year(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'month') <- month(docvars(guardian_corp, 'date'))
docvars(guardian_corp, 'week') <- week(docvars(guardian_corp, 'date'))
?corpus_subset
guardian_corp <- corpus_subset(guardian_corp, year >= 2016)
guardian_toks <- tokens(guardian_corp, remove_punct = TRUE)
# check sentiment
lsd_toks <- tokens_lookup(guardian_toks, data_dictionary_LSD2015[1:2])
head(lsd_toks, 2)
# DFM from classified tokens
lsd_dfm <- dfm(lsd_toks)
head(lsd_dfm, 5)
docvars(guardian_corp) %>% head
# more targeted sentiment analysis
eu <- c('EU', 'europ*', 'european union')
eu_toks <- tokens_keep(guardian_toks, phrase(eu), window = 10)
eu_toks[2]
eu_lsd_dfm <- dfm(eu_toks, dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = 'week', fill = TRUE)
matplot(eu_lsd_dfm, type = 'l', xaxt = 'n', lty = 1, ylab = 'Frequency')
?matplot
matplot(eu_lsd_dfm, type = 'l', xaxt = 'n', lty = 1, ylab = 'Frequency')
grid()
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
legend('topleft', col = 1:2, legend = c('Negative', 'Positive'), lty = 1, bg = 'white')
legend('topright', col = 1:2, legend = c('Negative', 'Positive'), lty = 1, bg = 'white')
# plot average sentiment
eu_n <- ntoken(dfm(eu_toks, group = docvars(eu_toks, 'week')))
plot((eu_lsd_dfm[,2] - eu_lsd_dfm[,1]) / eu_n,
type = 'l', ylab = 'Sentiment', xlab = '', xaxt = 'n')
axis(1, seq_len(ndoc(eu_lsd_dfm)), ymd("2016-01-01") + weeks(seq_len(ndoc(eu_lsd_dfm)) - 1))
grid()
abline(h = 0, lty = 2)
?ntoken
eu_n
head(eu_lsd_dfm)
